{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Student Performance Predictor - Model Training\n",
        "\n",
        "This notebook contains the complete pipeline for:\n",
        "1. Data Loading and Exploration\n",
        "2. Data Preprocessing\n",
        "3. Model Training and Comparison\n",
        "4. Model Evaluation\n",
        "5. Model Saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('../data/student_data.csv')\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"Dataset Info:\")\n",
        "df.info()\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nDataset Statistics:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check target variable distribution\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(df['final_result'].value_counts())\n",
        "print(\"\\nPercentage:\")\n",
        "print(df['final_result'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualize target distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['final_result'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
        "plt.title('Distribution of Final Results')\n",
        "plt.xlabel('Result')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation Heatmap\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of features by result\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "features = ['attendance_percentage', 'hours_studied', 'previous_score', \n",
        "            'assignments_submitted', 'internal_marks', 'age']\n",
        "\n",
        "for idx, feature in enumerate(features):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "    \n",
        "    df[df['final_result'] == 'Pass'][feature].hist(alpha=0.5, label='Pass', ax=axes[row, col], bins=20)\n",
        "    df[df['final_result'] == 'Fail'][feature].hist(alpha=0.5, label='Fail', ax=axes[row, col], bins=20)\n",
        "    axes[row, col].set_title(f'Distribution of {feature}')\n",
        "    axes[row, col].set_xlabel(feature)\n",
        "    axes[row, col].set_ylabel('Frequency')\n",
        "    axes[row, col].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Handle missing values (if any)\n",
        "print(\"Missing values before handling:\")\n",
        "print(df_processed.isnull().sum())\n",
        "\n",
        "# Fill missing values if any\n",
        "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "categorical_cols = ['gender', 'parent_education', 'internet_access']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_processed[col] = le.fit_transform(df_processed[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Encode target variable\n",
        "target_encoder = LabelEncoder()\n",
        "df_processed['final_result'] = target_encoder.fit_transform(df_processed['final_result'])\n",
        "\n",
        "print(\"\\nCategorical encoding completed.\")\n",
        "print(f\"Target encoding: {dict(zip(target_encoder.classes_, target_encoder.transform(target_encoder.classes_)))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df_processed.drop(['student_id', 'final_result'], axis=1)\n",
        "y = df_processed['final_result']\n",
        "\n",
        "print(\"Features:\", X.columns.tolist())\n",
        "print(\"\\nFeature shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"\\nTraining set target distribution:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "print(f\"\\nTest set target distribution:\")\n",
        "print(pd.Series(y_test).value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better handling\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(\"Feature scaling completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Train Accuracy': train_accuracy,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Model': model\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Train and evaluate all models\n",
        "results = []\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    result = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, name)\n",
        "    results.append(result)\n",
        "    trained_models[name] = result['Model']\n",
        "    print(f\"{name} - Test Accuracy: {result['Test Accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'Model'} for r in results])\n",
        "print(\"\\nModel Comparison Results:\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    \n",
        "    axes[row, col].bar(results_df['Model'], results_df[metric], color=['skyblue', 'lightgreen', 'salmon'])\n",
        "    axes[row, col].set_title(f'{metric} Comparison')\n",
        "    axes[row, col].set_ylabel(metric)\n",
        "    axes[row, col].set_ylim([0, 1])\n",
        "    axes[row, col].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(results_df[metric]):\n",
        "        axes[row, col].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Select Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model based on F1-Score\n",
        "best_model_idx = results_df['F1-Score'].idxmax()\n",
        "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"\\nBest Model Performance:\")\n",
        "print(results_df.loc[best_model_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed evaluation of best model\n",
        "y_pred_best = best_model.predict(X_test_scaled)\n",
        "\n",
        "print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
        "print(classification_report(y_test, y_pred_best, target_names=target_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=target_encoder.classes_, \n",
        "            yticklabels=target_encoder.classes_)\n",
        "plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Model and Preprocessing Objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save best model\n",
        "joblib.dump(best_model, '../models/best_model.pkl')\n",
        "joblib.dump(scaler, '../models/scaler.pkl')\n",
        "joblib.dump(label_encoders, '../models/label_encoders.pkl')\n",
        "joblib.dump(target_encoder, '../models/target_encoder.pkl')\n",
        "\n",
        "print(\"Model and preprocessing objects saved successfully!\")\n",
        "print(f\"\\nSaved files:\")\n",
        "print(\"- best_model.pkl\")\n",
        "print(\"- scaler.pkl\")\n",
        "print(\"- label_encoders.pkl\")\n",
        "print(\"- target_encoder.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance (for tree-based models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display feature importance for tree-based models\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=feature_importance, x='Importance', y='Feature')\n",
        "    plt.title(f'Feature Importance - {best_model_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(feature_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
